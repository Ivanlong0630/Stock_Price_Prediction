{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from datetime import datetime,timedelta\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "## Projects\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta,date,datetime\n",
    "import configparser as cp\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "import joblib\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "\n",
    "# Stats\n",
    "import math\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Airflow ##\n",
    "linear_analysis_arg={'owner':'airflow',\n",
    "                     'depends_on_past':False,\n",
    "                     'start_date':datetime.datetime(2022,3,1),\n",
    "                     'retries':1,\n",
    "                     'retry_delay':timedelta(minutes=10)\n",
    "                    }\n",
    "\n",
    "## Database ##\n",
    "# update to MySQL hook in Airflow\n",
    "config=cp.ConfigParser()\n",
    "config.read('/home/ubuntu/certi/db_login.txt')\n",
    "db_config=config['ivan_db']\n",
    "\n",
    "engine=create_engine('mysql+mysqlconnector://{0:s}:{1:s}@{2:s}/{3:s}'.format(db_config['userid'],\n",
    "                                                                             db_config['pwd'],\n",
    "                                                                             db_config['hostname'],\n",
    "                                                                             'STOCK_PRED'\n",
    "                                                                            ))\n",
    "stock_mapping=pd.read_sql(\"\"\"SELECT * \n",
    "                             FROM STOCK_PRED.NYSE_NASDAQ_TICKERS\n",
    "                          \"\"\",con=engine)\n",
    "print(stock_mapping.shape)\n",
    "print(stock_mapping.Symbol.nunique())\n",
    "\n",
    "\n",
    "## Regression ## \n",
    "def linear_reg_analysis_for(df):\n",
    "    lr_model=smf.ols('Close ~ DATE_ORDER',data=df).fit()\n",
    "    #lr_model=sm.OLS(x.Close,x.DATE_ORDER).fit()\n",
    "    \n",
    "    model_result={#'Stock':df.Stock[0],\n",
    "                  'R_squared':[lr_model.rsquared],\n",
    "                  'Coef':[lr_model.params[1]],\n",
    "                  'P_values':[lr_model.pvalues[1]],\n",
    "                  \n",
    "                  'Start_Date':df['Date'].min(),\n",
    "                  'End_Date':df['Date'].max(),\n",
    "                  'Num_records':[df.shape[0]],\n",
    "                  'Num_records_dist':[df.Date.nunique()]  \n",
    "                 }\n",
    "    \n",
    "    return pd.DataFrame(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Cleaning\n",
    "* Save the final result as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data_cleaning(file_path=''):\n",
    "    ## 1 Data Loading ##\n",
    "    # 1.1 Stock data - last 60 days\n",
    "    df=pd.read_sql(\"\"\"SELECT * \n",
    "                      FROM STOCK_PRED.ALL_STOCK_HIST\n",
    "                      WHERE DATE>=CURDATE()-INTERVAL 60 DAY\n",
    "                             \"\"\",\n",
    "                   con=engine)\n",
    "    print(df.shape)\n",
    "    print(df['Date'].max(),df['Date'].min())\n",
    "    \n",
    "    if df.loc[df.Date==df.Date.max(),:].shape[0]>5000:\n",
    "        ## 2 Data Cleaning ##\n",
    "        # 2.1 Remove NAs\n",
    "        df_1=df.dropna(axis=0,how='any')\n",
    "    \n",
    "        # 2.2 Remove accounts with Negative Stock price\n",
    "        negative_stocks=df_1.loc[df_1.Close<0,'Stock'].unique()\n",
    "        df_1=df_1.loc[~df_1.Stock.isin(negative_stocks),:]\n",
    "    \n",
    "        # 2.3 Keep active stocks\n",
    "        active_stocks=df_1.loc[df_1.Date==df_1.Date.max(),'Stock'].to_list()\n",
    "        df_2=df_1.loc[df_1.Stock.isin(active_stocks),:].reset_index(drop=True)\n",
    "    \n",
    "        # 2.4 Add DATE_ORDER\n",
    "        df_2.loc[:,'DATE_ORDER']=df_2.groupby('Stock').Date.transform(lambda x:x.rank(method='first',ascending=True))\n",
    "    \n",
    "        # 2.4 Merging\n",
    "        df_3=pd.merge(df_2,\n",
    "                      stock_mapping.loc[:,['Symbol','Name','Country','IPOYear','Sector','Industry']],\n",
    "                      how='left',\n",
    "                      left_on='Stock',\n",
    "                      right_on='Symbol'\n",
    "                     )\n",
    "        df_3.drop(['Symbol'],axis=1,inplace=True)\n",
    "        df_3.rename(columns={'DAY_ORDER':'DATE_ORDER'},inplace=True)\n",
    "        df_3.sort_values(by=['Stock','Date'],ascending=True,inplace=True)\n",
    "    \n",
    "        ## 3. Saving ##\n",
    "        joblib.dump(df_3,file_path)\n",
    "        \n",
    "    else:\n",
    "        print('Data issue: latest data {:s} only as {:,.0f} rows'.format(df.Date.max().strftime('%Y-%m-%d'),\n",
    "                                                                         df.loc[df.Date==df.Date.max(),:].shape[0]\n",
    "                                                                        ))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impl_linear_reg(file_path=''):\n",
    "    ## 1 Data Loading & Preprocessing ##\n",
    "    df_3=joblib.load(file_path)\n",
    "    print(df_3.shape)\n",
    "    print(df_3.Date.max())\n",
    "    \n",
    "    ## 2 Modeling Implementing ##\n",
    "    linear_reg_sum=df_3.groupby(['Stock']).apply(linear_reg_analysis_for).reset_index(drop=False)\n",
    "    print(linear_reg_sum.shape)\n",
    "    \n",
    "    ## 3. Processing ##\n",
    "    # 3.1 Add new columns\n",
    "    linear_reg_sum.loc[:,'WT_Coef']=linear_reg_sum.R_squared*linear_reg_sum.Coef\n",
    "    linear_reg_sum.loc[:,'Model_date']=datetime.now(tz=pytz.utc).astimezone(timezone('US/Pacific'))\n",
    "    \n",
    "    # 3.2 Additional tables\n",
    "    stock_strt_end_price=df_3.groupby('Stock').agg(start_price=('Close','first'),\n",
    "                                                   end_price=('Close','end')\n",
    "                                                  ).reset_index(drop=False)\n",
    "    linear_reg_sum_2=pd.merge(linear_reg_sum,\n",
    "                              stock_strt_end_price,\n",
    "                              how='left',\n",
    "                              on='Stock'\n",
    "                             ).assign(growth_rate=lambda x:(x.end_price-x.start_price)/x.start_price)\n",
    "    \n",
    "    linear_reg_sum_2=pd.merge(linear_reg_sum_2,\n",
    "                              stock_mapping.loc[:,['Symbol','Name','Industry','SE']],\n",
    "                              how='left',\n",
    "                              left_on='Stock',\n",
    "                              right_on='Symbol')\n",
    "    \n",
    "    linear_reg_sum_2.drop('level_1',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-02 06:12:27.783721+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 3, 1, 22, 12, 27, 783721, tzinfo=<DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_date=datetime.now(tz=pytz.utc)\n",
    "print(curr_date)\n",
    "curr_date.astimezone(timezone('US/Pacific'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
